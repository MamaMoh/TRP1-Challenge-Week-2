{
  "rubric_metadata": {
    "rubric_name": "Week 2: The Automaton Auditor Self-Evaluation",
    "grading_target": "Week 2 Auditor Repository & Architectural Report",
    "version": "3.0.0"
  },
  "dimensions": [
    {
      "id": "git_forensic_analysis",
      "name": "Git Forensic Analysis",
      "target_artifact": "github_repo",
      "forensic_instruction": "Run 'git log --oneline --reverse' on the cloned repository. Count the total number of commits. Check if the commit history tells a progression story: Environment Setup -> Tool Engineering -> Graph Orchestration. Extract all commit messages and timestamps. Flag if there is a single 'init' commit or a 'bulk upload' pattern with no iterative development.",
      "success_pattern": "More than 3 commits showing clear progression from setup to tool engineering to graph orchestration. Atomic, step-by-step history with meaningful commit messages.",
      "failure_pattern": "Single 'init' commit or bulk upload of all code at once. No iterative development visible. Timestamps clustered within minutes.",
      "judicial_logic": {
        "prosecutor": "If the commit history shows a single 'init' commit or bulk upload, charge with 'Development Process Fraud'. Score 1. If timestamps are clustered within minutes, charge with 'Rushed Implementation'.",
        "defense": "Highlight any evidence of iterative development, even if commits are not perfectly atomic. Reward effort shown in commit messages that tell a story of learning and refinement.",
        "tech_lead": "Assess the progression narrative. Does the git history demonstrate a logical build-up from infrastructure to tools to orchestration? Evaluate the engineering process quality."
      }
    },
    {
      "id": "state_management_rigor",
      "name": "State Management Rigor",
      "target_artifact": "github_repo",
      "forensic_instruction": "Scan for 'src/state.py' or equivalent state definitions in 'src/graph.py'. Use AST parsing (not regex) to find classes inheriting from 'BaseModel' (Pydantic) or 'TypedDict'. Verify that the state actively maintains a collection of 'Evidence' objects and a list of 'JudicialOpinion' objects. Check for the use of 'operator.add' and 'operator.ior' as state reducers in 'Annotated' type hints to prevent data overwriting during parallel execution. Capture the full code snippet of the core 'AgentState' definition.",
      "success_pattern": "'AgentState' uses TypedDict or BaseModel with Annotated reducers. 'Evidence' and 'JudicialOpinion' are Pydantic BaseModel classes with typed fields. Reducers like 'operator.add' (for lists) and 'operator.ior' (for dicts) are present.",
      "failure_pattern": "Plain Python dicts used for state. No Pydantic models. No reducers, meaning parallel agents will overwrite each other's data.",
      "judicial_logic": {
        "prosecutor": "If plain Python dicts are used for state without Pydantic models, charge with 'Type Safety Negligence'. If reducers are missing, charge with 'Parallel Execution Hazard'. Score max 2.",
        "defense": "Highlight any creative use of Pydantic validation or type hints, even if not perfect. Reward architectural thinking about state management.",
        "tech_lead": "Assess the state schema design. Are reducers correctly applied? Is the state structure maintainable and extensible? Evaluate technical debt."
      }
    },
    {
      "id": "graph_orchestration",
      "name": "Graph Orchestration Architecture",
      "target_artifact": "github_repo",
      "forensic_instruction": "Scan for the 'StateGraph' builder instantiation in 'src/graph.py'. Use AST parsing to analyze 'builder.add_edge()' and 'builder.add_conditional_edges()' calls. Determine if the Detectives (RepoInvestigator, DocAnalyst, VisionInspector) branch out from a single node and run concurrently (fan-out). Verify there is a synchronization node ('EvidenceAggregator' or equivalent) that collects all evidence before the Judges are invoked (fan-in). Verify the Judges (Prosecutor, Defense, TechLead) also fan-out in parallel from the aggregation node and fan-in before the ChiefJustice. Check for conditional edges that handle 'Evidence Missing' or 'Node Failure' scenarios. Capture the specific Python block defining the graph's nodes and edges.",
      "success_pattern": "Two distinct parallel fan-out/fan-in patterns: one for Detectives, one for Judges. Conditional edges handle error states. Graph structure: START -> [Detectives in parallel] -> EvidenceAggregator -> [Judges in parallel] -> ChiefJustice -> END.",
      "failure_pattern": "Purely linear flow (RepoInvestigator -> DocAnalyst -> Judge -> End). No parallel branches. No synchronization node. No conditional edges for error handling.",
      "judicial_logic": {
        "prosecutor": "If the graph is purely linear (A->B->C), charge with 'Orchestration Fraud'. Score 1. If parallel branches exist but no synchronization node, charge with 'Race Condition Risk'.",
        "defense": "Support simpler graph designs if they implement robust 'State' transitions and Pydantic validation at every node. Reward architectural thinking even if parallelism is limited.",
        "tech_lead": "Determine if the fan-in synchronization correctly aggregates lists of evidence before passing them to the judicial bench. Evaluate the graph's scalability and maintainability."
      }
    },
    {
      "id": "safe_tool_engineering",
      "name": "Safe Tool Engineering",
      "target_artifact": "github_repo",
      "forensic_instruction": "Scan 'src/tools/' for the repository cloning logic. Verify that 'tempfile.TemporaryDirectory()' or equivalent sandboxing is used for git clone operations. Check for raw 'os.system()' calls -- these are a security violation. Verify that 'subprocess.run()' or equivalent is used with proper error handling (capturing stdout/stderr, checking return codes). Ensure the cloned repo path is never the live working directory. Check that git authentication errors are handled gracefully. Capture the specific Python function responsible for executing the repository clone.",
      "success_pattern": "All git operations run inside 'tempfile.TemporaryDirectory()'. 'subprocess.run()' used with error handling. No raw 'os.system()' calls. Authentication failures caught and reported.",
      "failure_pattern": "Raw 'os.system(\"git clone <url>\")' drops code into the live working directory. No error handling around shell commands. No input sanitization on the repo URL.",
      "judicial_logic": {
        "prosecutor": "If tool execution relies on raw 'os.system' without error handling or sandboxing, charge with 'Security Negligence'. Score 1. If cloned code is placed in the live working directory, charge with 'Contamination Risk'.",
        "defense": "Highlight any creative use of sandboxing or error handling, even if not perfect. Reward security-conscious thinking.",
        "tech_lead": "Assess the tool safety architecture. Are operations properly isolated? Is error handling comprehensive? Evaluate security posture and maintainability."
      }
    },
    {
      "id": "structured_output_enforcement",
      "name": "Structured Output Enforcement",
      "target_artifact": "github_repo",
      "forensic_instruction": "Scan Judge nodes in 'src/nodes/judges.py'. Verify that LLMs are invoked using '.with_structured_output()' or '.bind_tools()' bound to the Pydantic 'JudicialOpinion' schema. Check that the output includes 'score' (int), 'argument' (str), and 'cited_evidence' (list). Verify there is retry logic or error handling if a Judge returns freeform text instead of structured JSON. Capture the code block responsible for querying the Judge LLMs.",
      "success_pattern": "All Judge LLM calls use '.with_structured_output(JudicialOpinion)' or equivalent. Retry logic exists for malformed outputs. Output is validated against the Pydantic schema before being added to state.",
      "failure_pattern": "Judge nodes call LLMs with plain prompts and parse freeform text responses. No Pydantic validation on output. No retry on parse failure.",
      "judicial_logic": {
        "prosecutor": "If Judge nodes return freeform text without structured output enforcement, charge with 'Hallucination Liability'. Score max 2. If no retry logic exists, charge with 'Error Handling Negligence'.",
        "defense": "Highlight any creative use of structured output or validation, even if not perfect. Reward architectural thinking about output reliability.",
        "tech_lead": "Assess the structured output implementation. Is validation comprehensive? Is error handling robust? Evaluate the system's reliability and maintainability."
      }
    },
    {
      "id": "judicial_nuance",
      "name": "Judicial Nuance and Dialectics",
      "target_artifact": "github_repo",
      "forensic_instruction": "Scan 'src/nodes/judges.py' or prompt templates. Verify that Prosecutor, Defense, and Tech Lead personas have distinct, conflicting system prompts. Compare the three prompts -- if they share more than 50% of text, flag as 'Persona Collusion'. Check if the Prosecutor prompt includes adversarial language and instructions to look for gaps, security flaws, and laziness. Check if the Defense prompt includes instructions to reward effort, intent, and creative workarounds. Check if the Tech Lead prompt focuses on architectural soundness, maintainability, and practical viability. Verify the graph forces all three judges to run in parallel on the same evidence for each criterion.",
      "success_pattern": "Three clearly distinct personas with conflicting philosophies. Prompts actively instruct the model to be adversarial (Prosecutor), forgiving (Defense), or pragmatic (Tech Lead). Judges produce genuinely different scores and arguments for the same evidence.",
      "failure_pattern": "Single agent acts as 'The Grader' with no persona separation. Or three judges exist but share 90% of prompt text, producing near-identical outputs. Scores are random or purely praise/criticism without nuance.",
      "judicial_logic": {
        "prosecutor": "If the three judges share 90% of the same prompt text, charge with 'Persona Collusion'. Score max 2. If outputs are free text, charge with 'Hallucination Liability'.",
        "defense": "Look for specific prompt instructions that force the model to be 'contrarian' or 'forgiving'. Reward any evidence of dialectical thinking, even if implementation is imperfect.",
        "tech_lead": "Evaluate if the judges successfully map their opinions back to specific rubric criteria IDs. Assess the quality of the dialectical process and its effectiveness."
      }
    },
    {
      "id": "chief_justice_synthesis",
      "name": "Chief Justice Synthesis Engine",
      "target_artifact": "github_repo",
      "forensic_instruction": "Scan 'src/nodes/justice.py' for the ChiefJusticeNode implementation. Verify the conflict resolution uses hardcoded deterministic Python logic, not just an LLM prompt. Check for these specific rules: (1) Rule of Security -- if the Prosecutor identifies a confirmed security vulnerability, the score is capped at 3 regardless of Defense arguments. (2) Rule of Evidence -- if the Defense claims 'Deep Metacognition' but Detective evidence shows the artifact is missing, the Defense is overruled. (3) Rule of Functionality -- if the Tech Lead confirms the architecture is modular, this carries the highest weight for the Architecture criterion. Check if score variance > 2 triggers a specific re-evaluation rule. Verify the output is a structured Markdown report, not a console print.",
      "success_pattern": "Deterministic Python if/else logic implementing named rules (security override, fact supremacy, functionality weight). Score variance triggers specific re-evaluation. Output is a Markdown file with Executive Summary, Criterion Breakdown (with dissent), and Remediation Plan.",
      "failure_pattern": "ChiefJustice is just another LLM prompt that averages the three judge scores. No hardcoded rules. No dissent summary. Output is console text or unstructured.",
      "judicial_logic": {
        "prosecutor": "If ChiefJustice is just an LLM prompt averaging scores, charge with 'Synthesis Fraud'. Score 1. If no hardcoded rules exist, charge with 'Determinism Negligence'.",
        "defense": "Highlight any creative use of conflict resolution logic, even if not perfect. Reward architectural thinking about synthesis.",
        "tech_lead": "Assess the synthesis engine design. Are rules deterministic and auditable? Is the output structured and actionable? Evaluate the system's reliability and maintainability."
      }
    },
    {
      "id": "theoretical_depth",
      "name": "Theoretical Depth (Documentation)",
      "target_artifact": "pdf_report",
      "forensic_instruction": "Search the PDF report for these specific terms: 'Dialectical Synthesis', 'Fan-In / Fan-Out', 'Metacognition', 'State Synchronization'. Determine if the term appears in a substantive architectural explanation or is just a buzzword dropped in the executive summary. Check if the report explains HOW the architecture executes these concepts, not just that they exist. Flag terms that appear without supporting explanation as 'Keyword Dropping'.",
      "success_pattern": "Terms appear in detailed architectural explanations. The report explains how Dialectical Synthesis is implemented via three parallel judge personas. Fan-In/Fan-Out is tied to specific graph edges. Metacognition is connected to the system evaluating its own evaluation quality.",
      "failure_pattern": "Terms appear only in the executive summary or introduction. No connection to actual implementation. 'We used Dialectical Synthesis' with no explanation of how.",
      "judicial_logic": {
        "prosecutor": "If the report claims features that are not present in the code, charge with 'Auditor Hallucination'. Score 1. If terms are just keywords without explanation, charge with 'Buzzword Dropping'.",
        "defense": "Identify sections where the trainee demonstrates deep alignment with Multi-Agent System theories. Reward substantive explanations even if implementation is imperfect.",
        "tech_lead": "Verify if the architectural notes provide enough detail for a third party to recreate the swarm. Assess the quality of theoretical understanding and its connection to implementation."
      }
    },
    {
      "id": "report_accuracy",
      "name": "Report Accuracy (Cross-Reference)",
      "target_artifact": "pdf_report",
      "forensic_instruction": "Extract all file paths mentioned in the PDF report (e.g., 'We isolated the AST logic in src/tools/ast_parser.py', 'We implemented parallel Judges in src/nodes/judges.py'). Cross-reference each claimed file path against the evidence collected by the RepoInvestigator. Build two lists: (1) Verified Paths -- files that the report mentions and actually exist in the repo. (2) Hallucinated Paths -- files the report claims exist but the RepoInvestigator found no evidence of. Flag any claims about features (e.g., 'We implemented parallel Judges') where the code evidence contradicts the claim.",
      "success_pattern": "All file paths mentioned in the report exist in the repo. Feature claims match code evidence. Zero hallucinated paths.",
      "failure_pattern": "Report references files that do not exist. Claims parallel execution but code shows linear flow. Multiple hallucinated paths detected.",
      "judicial_logic": {
        "prosecutor": "If the report references non-existent files, charge with 'Hallucination'. Score 1. If feature claims contradict code evidence, charge with 'Misrepresentation'.",
        "defense": "Highlight any accurate cross-referencing or honest acknowledgment of limitations. Reward accuracy and transparency.",
        "tech_lead": "Assess the alignment between documentation and implementation. Is the report accurate and verifiable? Evaluate the quality of documentation."
      }
    },
    {
      "id": "swarm_visual",
      "name": "Architectural Diagram Analysis",
      "target_artifact": "pdf_images",
      "forensic_instruction": "Extract images from the PDF report. Classify each diagram: is it an accurate LangGraph State Machine diagram, a sequence diagram, or just generic flowchart boxes? Check if the diagram explicitly visualizes the parallel split: START -> [Detectives in parallel] -> Evidence Aggregation -> [Prosecutor || Defense || TechLead in parallel] -> Chief Justice Synthesis -> END. Verify the diagram distinguishes between parallel branches and sequential steps. Flag diagrams that show a simple linear pipeline as 'Misleading Architecture Visual'.",
      "success_pattern": "Diagram accurately represents the StateGraph with clear parallel branches for both Detectives and Judges. Fan-out and fan-in points are visually distinct. Flow matches the actual code architecture.",
      "failure_pattern": "Generic box-and-arrow diagram with no indication of parallelism. Or no diagram present at all. Diagram shows linear flow that contradicts the parallel architecture claimed in the report.",
      "judicial_logic": {
        "prosecutor": "If the diagram shows linear flow contradicting parallel claims, charge with 'Misleading Visual'. Score 1. If no diagram exists, charge with 'Documentation Incomplete'.",
        "defense": "Highlight any creative visual representation or effort to document architecture. Reward visual communication even if not perfect.",
        "tech_lead": "Assess the quality of architectural visualization. Does the diagram accurately represent the system? Evaluate the clarity and accuracy of visual documentation."
      }
    }
  ],
  "synthesis_rules": {
    "security_override": "Confirmed security flaws (e.g., shell injection in git tools, raw os.system with unsanitized inputs) cap the total score at 3, overriding any effort points from the Defense.",
    "fact_supremacy": "Forensic evidence (facts from Detectives) always overrules Judicial opinion (interpretation from Judges). If the Defense claims 'Deep Metacognition' but the RepoInvestigator found no supporting code, the Defense is overruled for hallucination.",
    "functionality_weight": "If the Tech Lead confirms the architecture is modular and workable, this carries the highest weight for the 'Graph Orchestration Architecture' criterion.",
    "dissent_requirement": "The Chief Justice must summarize why the Prosecutor and Defense disagreed in the final report. Every criterion with a score variance > 2 must include an explicit dissent explanation.",
    "variance_re_evaluation": "If score variance across the three judges exceeds 2 for any criterion (e.g., Prosecutor says 1, Defense says 5), trigger a re-evaluation of the specific evidence cited by each judge before rendering the final score."
  }
}
